dim(tw)
tw <- []
remove(tw)
x <- c('nfl')
n <- 10000
tw <- rbind(tw,getData(n, x))
dim(tw)
tw <- getData(n, x)
colnames(tw)
dim(tw)
summary(tw$lang)
tw$lang <- as.factor(tw$lang)
summary(tw$lang)
remove(tw)
x <- c('#plannedparenthood', '#pp')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('planned parenthood', '#pp')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
head(tw$text)
together<- 0
library(textclean)
library(rtweet)
library(httpuv)
library(twitteR)
#DO NOT GIVE OUT THE FOLLOWING INFORMATION TO ANYONE
consumer_key <- 	'3bxyiuG3DJzmXlUvAF6NRQpv5'
consumer_secret <- 'i3Td2j5iQToDCcSTm0kLshb6vIIETutoT2YHYY40kbPt9oqr4V'
access_token <- "953818124243087360-kEijMPCzceyjWH3D9Hf8BSImq9K3ihp"
access_secret <- "GjDCLKyT1klUeRVJoEsSH74z7tFt8fEc5cj9Ks3YYp6vs"
#This authorizes your computer to receive the information from twitter
# This should only be run once per R session
twitter_tokens <- create_token(app = "API Tester App for MAT 552",consumer_key = consumer_key, consumer_secret = consumer_secret)
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#Call to twitter for tweets containing specific hashtags or words
#n is the number of tweets requested for each term
x <- c('planned parenthood', '#pp')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
getData <- function(n= 10000, x){
#this x is the requested terms to search for separated by a comma and in quotes
#do you want to search for each term separately or together? 1 is yes 0 is no
together<- 0
#do you want the tweets cleaned of punctuation and make everything lowercase
clean<-1
string<-x[1]
for(i in 2:length(x)){
string<-paste(string,'+',x[i])
}
#This function searches twitter for tweets containing all requested terms from the last 10 days
tw<-NULL
if(together==1){
for(i in 1:length(x)){
new.tweets<-search_tweets(x[i],n=n,full_text='extended')
if(clean>0){
for(j in 1:nrow(new.tweets)){
str<-as.character(new.tweets[j,5])
str <- gsub('[[:punct:]]','',str)
str<-replace_emoji_identifier(str,emoji_dt = lexicon::hash_emojis_identifier)
str<-replace_emoji(str,emoji_dt = lexicon::hash_emojis)
str<-replace_emoticon(str, emoticon_dt = lexicon::hash_emoticons)
str<-gsub("<...","",str)
str<-gsub(" http.*","",str)
new.tweets[j,5]<-str
}
}
tw<-rbind(tw,new.tweets)
if(nrow(new.tweets)==n){print(paste('more tweets of: ',x[i],' may be possible'))}
}}else{
tw<-search_tweets(string,n=n,full_text='extended')
if(clean>0){
for(j in 1:nrow(tw)){
str<-as.character(tw[j,5])
str <- gsub('[[:punct:]]','',str)
str<-replace_emoji_identifier(str,emoji_dt = lexicon::hash_emojis_identifier)
str<-replace_emoji(str,emoji_dt = lexicon::hash_emojis)
str<-replace_emoticon(str, emoticon_dt = lexicon::hash_emoticons)
str<-gsub("<...","",str)
str<-gsub(" http.*","",str)
tw[j,5]<-str
}
}
}
return(tw)
}
consumer_key <- 	'3bxyiuG3DJzmXlUvAF6NRQpv5'
consumer_secret <- 'i3Td2j5iQToDCcSTm0kLshb6vIIETutoT2YHYY40kbPt9oqr4V'
access_token <- "953818124243087360-kEijMPCzceyjWH3D9Hf8BSImq9K3ihp"
access_secret <- "GjDCLKyT1klUeRVJoEsSH74z7tFt8fEc5cj9Ks3YYp6vs"
#This authorizes your computer to receive the information from twitter
# This should only be run once per R session
twitter_tokens <- create_token(app = "API Tester App for MAT 552",consumer_key = consumer_key, consumer_secret = consumer_secret)
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#Call to twitter for tweets containing specific hashtags or words
#n is the number of tweets requested for each term
x <- c('planned parenthood', '#pp')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('planned parenthood OR #pp OR #standwithpp')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
remove(tw)
x <- c('planned parenthood OR #pp OR #standwithpp')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
tw
clear
clear()
dim(tw)
x <- c('planned parenthood OR #pp')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('planned parenthood OR #pp OR #standwithpp')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('planned parenthood OR #pp OR #standwithpp OR pp OR pregnancy')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
pi
9/16
8/9
x <- c('planned parenthood OR #pp OR #standwithpp OR pp OR pregnancy OR #IstandwithPP')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('planned parenthood OR pp OR #standwithpp OR pp OR pregnancy OR #IstandwithPP')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('pp')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
x <- c('pp OR planned parenthood')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('pp OR planned parenthood')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('pp', 'planned parenthood')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
x <- c('#pp', 'istandwithpp', 'standwithpp', 'planned parenthood')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('#pp', 'istandwithpp')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('#pp', '#istandwithpp', '#standwithpp', 'planned parenthood')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('pregnancy')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('pregnancy OR planned parenthood OR trump')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
getData <- function(n= 10000, x){
#this x is the requested terms to search for separated by a comma and in quotes
#do you want the tweets cleaned of punctuation and make everything lowercase
clean<-1
string<-x[1]
for(i in 2:length(x)){
string<-paste(string,'+',x[i])
}
#This function searches twitter for tweets containing all requested terms from the last 10 days
tw<-NULL
new.tweets<-search_tweets(x,n=n,full_text='extended')
if(clean>0){
for(j in 1:nrow(new.tweets)){
str<-as.character(new.tweets[j,5])
str <- gsub('[[:punct:]]','',str)
str<-replace_emoji_identifier(str,emoji_dt = lexicon::hash_emojis_identifier)
str<-replace_emoji(str,emoji_dt = lexicon::hash_emojis)
str<-replace_emoticon(str, emoticon_dt = lexicon::hash_emoticons)
str<-gsub("<...","",str)
str<-gsub(" http.*","",str)
new.tweets[j,5]<-str
}
}
tw<-rbind(tw,new.tweets)
if(nrow(new.tweets)==n){print(paste('more tweets of: ',x[i],' may be possible'))}
return(tw)
}
#DO NOT GIVE OUT THE FOLLOWING INFORMATION TO ANYONE
consumer_key <- 	'3bxyiuG3DJzmXlUvAF6NRQpv5'
consumer_secret <- 'i3Td2j5iQToDCcSTm0kLshb6vIIETutoT2YHYY40kbPt9oqr4V'
access_token <- "953818124243087360-kEijMPCzceyjWH3D9Hf8BSImq9K3ihp"
access_secret <- "GjDCLKyT1klUeRVJoEsSH74z7tFt8fEc5cj9Ks3YYp6vs"
#This authorizes your computer to receive the information from twitter
# This should only be run once per R session
twitter_tokens <- create_token(app = "API Tester App for MAT 552",consumer_key = consumer_key, consumer_secret = consumer_secret)
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#Call to twitter for tweets containing specific hashtags or words
#n is the number of tweets requested for each term
x <- c('pregnancy OR planned parenthood OR trump')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
x <- c('pregnancy OR planned parenthood OR #istandwithpp OR #standwithpp')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
load('tweets_an.rda')
setwd("~/GitHub/UnplannedPregnancy/TwitterAnalysis")
load('tweets_an.rda')
r
help
library(ranger)
install.packages('ranger')
setwd("~/GitHub/capstone-spring-2018-team-2")
library(ranger)
load('data.rda')
colnames(rd.p)
# load in meta data
load(meta.rda)
meta_df['ADRISK42']
load('meta.rda')
meta_df['ADRISK42']
meta_df <- load('meta.rda')
# find description using meps code
meta_df['ADRISK42']
target <- 'ERTOT15'
y <- rd.p[,target]
library(ranger)
load('data.rda')
target <- 'ERTOT15'
y <- rd.p[,target]
y <- as.data.frame(rd.p[,target])
x <- rd.p[,-target]
y <- as.data.frame(rd.p[,target])
x <- rd.p[,-which(names(rd.p) == target)]
library(caret)
trainidx <- createDataPartition(rd.p, p=.8, list = FALSE)
trainidx <- createDataPartition(rd.p$ERTOT15, p=.8, list = FALSE)
library(caret)
load('data.rda')
target <- 'ERTOT15'
trainidx <- createDataPartition(rd.p$ERTOT15, p=.8, list = FALSE)
y.train <- as.data.frame(rd.p[trainidx,target])
x.train <- rd.p[trainidx,-which(names(rd.p) == target)]
y.test <- as.data.frame(rd.p[-trainidx,target])
x.test <- rd.p[-trainidx,-which(names(rd.p) == target)]
names(rd.p) == target
which(names(rd.p) == target)
names(rd.p[,names(rd.p) != target]
)
f <- formula(paste(target, paste(names(rd.p[,names(rd.p) != target], collapse = '+' ), sep = '~'))
f <- formula(paste(target, paste(names(rd.p[,names(rd.p) != target]), collapse = '+' ), sep = '~'))
f <- formula(paste(target, paste(names(rd.p[,names(rd.p) != target]), collapse = '+' ), sep = '~'))
train <- rd.p[trainidx,]
f <- formula(paste(target, paste(names(rd.p[,names(rd.p) != target]), collapse = '+' ), sep = '~'))
fit <- ranger(formula = f,
data = train,
num.trees = 50,
importance = 'impurity',
min.node.size = 30,
sample.fraction = .8)
preds <- predict(fir, x.test)
preds <- predict(fit, x.test)
preds <- predict(fit, x.test)$predictions
preds <- as.data.frame(predict(fit, x.test)$predictions)
fit$variable.importance
imp <- fit$variable.importance
imp <- as.data.frame(fit$variable.importance)
summary(imp)
imp <- fit$variable.importance
plot(imp)
plot(imp>50)
plot(imp[imp>50])
varImpPlot(fit)
library(caret)
plot(fit$variable.importance)
imp <- as.table(imp)
plot(imp)
imp <- fit$variable.importance
imp <- imp[imp>50]
imp <- as.table(imp)
plot(imp)
plot(imp) + coord_flip()
importance(fit)
fit <- fit[fit > 50]
imp <- imp[imp > 50]
imp
plot(imp)
imp <- fit$variable.importance
imp <- imp[imp > 50]
plot(imp,xaxt="n")
axis(1,at=1:3,labels=names(imp))
plot(imp,xaxt="n")
axis(1,at=1:len(imp),labels=names(imp))
length(imp)
axis(1,at=1:length(imp),labels=names(imp))
barplot(imp)
barplot(imp, horiz=TRUE )
barplot(imp, horiz=TRUE, las= 1)
View(train)
load("~/GitHub/capstone-spring-2018-team-2/meta.rda")
load("~/GitHub/capstone-spring-2018-team-2/meta.rda")
load("~/GitHub/capstone-spring-2018-team-2/meta.rda")
load("~/GitHub/capstone-spring-2018-team-2/meta.rda")
meps_scrape = function(){
library('rvest')
load("data.rda")
#Specifying the url for desired website to be scrapped
url = "utils/files/meps_codes.html"
#Reading the HTML code from the website
webpage = read_html(url)
#Using CSS selectors to scrap the rankings section
table_data_html = html_nodes(webpage,'.codes')
#Converting the table data to text
table_data = html_text(table_data_html)
# convert html table to a dataframe
codes_df = html_table(table_data_html, header = F, trim = T)[[1]]
rownames(codes_df) = codes_df$X1
codes_df$X1 = NULL
# map the variable description in column 'X4' to the codes in rd.p
meta_named_char = sapply(colnames(rd.p), function(x) codes_df[x, 'X4'] )
meta = data.frame(as.list(descriptions))
save(meta, file = "meta.rda")
}
meps_scrape()
install.packages('rvest')
meps_scrape()
load("~/GitHub/capstone-spring-2018-team-2/meta.rda")
target <- 'ERTOT15'
meta[target]
library(ranger)
library(caret)
load('data.rda')
target <- 'ERTOT15'
#split
trainidx <- createDataPartition(rd.p$ERTOT15, p=.8, list = FALSE)
train <- rd.p[trainidx,]
y.test <- as.data.frame(rd.p[-trainidx,target])
x.test <- rd.p[-trainidx,-which(names(rd.p) == target)]
f <- formula(paste(target, paste(names(rd.p[,names(rd.p) != target]), collapse = '+' ), sep = '~'))
fit <- ranger(formula = f,
data = train,
num.trees = 50,
importance = 'impurity',
min.node.size = 30,
sample.fraction = .8)
preds <- as.data.frame(predict(fit, x.test)$predictions)
varImpPlot(fit)
imp <- fit$variable.importance
imp <- imp[imp > 50]
barplot(imp, horiz=TRUE, las= 1)
imp
meta['TOTTCH15']
library(foreign)
link <- curl(ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/NSFG/spss/2013_2015_FemPregSetup.sps)
download.file("ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/NSFG/spss/2013_2015_FemPregSetup.sps", temp <- tempfile())
unzipped_file = unzip(temp)
rd = read.xport(unzipped_file)
unlink(temp)
download.file("ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/NSFG/2013_2015_FemPregData.dat", temp <- tempfile())
unzipped_file = unzip(temp)
setwd("~/GitHub/UnplannedPregnancy")
rd <- read.csv("CDC_PRAMStat_Data_for_2011.csv")
colnames(rd)
levels(rd$Question)
colnames(rd)
levels(rd$Response)
colnames(rd)
save(rd, file = 'data.rda')
summary(rd$Data_Value)
levels(rd$DataSource)
head(rd)
levels(rd$Data_Value_Type)
rd[rd$Question == levels(rd$Question)[1],]
colnames(rd)
levels(rd$LocationAbbr)
levels(rd$Geolocation)
colnames(rd)
View(rd)
length(levels(rd$Break_Out))
length(levels(rd$Break_Out_Category))
levels(rd$Question)
length(levels(rd$Question))
length(levels(rd$QuestionId))
un <- rd[rd$Break_Out == 'Unintended',]
length(levels(un$QuestionId))
load('data.rda')
unintended <- rd[rd$Break_Out == 'Unintended',]
levels(rd$Question)
intended <- rd[rd$Break_Out == 'Intended',]
unintended.bp <- unintended[unintended$Question == "(*PCH) During the 3 months before you got pregnant with your new baby  did you have high blood pressure (hypertension)?",]
levels(unintended.bp$QuestionId)
levels(droplevel(unintended.bp$QuestionId))
levels(droplevels(unintended.bp$QuestionId))
unintended.bp <- unintended[unintended$QuestionID == 'QUO130',]
levels(droplevels(unintended$Question))
length(levels(droplevels(unintended$Question)))
unintended.bp <- unintended[unintended$QuestionID == 'QUO130',]
unintended.bp <- unintended[unintended$QuestionId == 'QUO130',]
View(unintended.bp)
level(unintended.bp$Response)
levels(unintended.bp$Response)
unintended.bp <- droplevels(unintended[unintended$QuestionId == 'QUO130',])
levels(unintended.bp$Response)
? aggregate
tab <- aggregate(unintended.bp$Sample_Size, by = unintended.bp$Response, FUN = sum)
tab <- aggregate(unintended.bp$Sample_Size, by = levels(unintended.bp$Response), FUN = sum)
tab <- aggregate(unintended.bp$Sample_Size, by = as.list(levels(unintended.bp$Response)), FUN = sum)
tab <- aggregate(unintended.bp, by = as.list(levels(unintended.bp$Response)), FUN = sum)
tab <- aggregate(unintended.bp, by = as.list(unintended.bp$Response), FUN = sum)
tab <- aggregate(unintended.bp$Sample_Size, by = as.list(Response = unintended.bp$Response), FUN = sum)
tab <- aggregate(x =unintended.bp$Sample_Size, by = as.list(Response = unintended.bp$Response), FUN = sum)
tab <- aggregate(unintended.bp$Sample_Size, by = list(Response = unintended.bp$Response), FUN = sum)
tab
untab <- aggregate(unintended.bp$Sample_Size, by = list(Response = unintended.bp$Response), FUN = sum)
tab <- aggregate(intended.bp$Sample_Size, by = list(Response = intended.bp$Response), FUN = sum)
unintended.bp <- droplevels(unintended[unintended$QuestionId == 'QUO130',])
intended.bp <- droplevels(intended[intended$QuestionId == 'QUO130',])
untab <- aggregate(unintended.bp$Sample_Size, by = list(Response = unintended.bp$Response), FUN = sum)
tab <- aggregate(intended.bp$Sample_Size, by = list(Response = intended.bp$Response), FUN = sum)
untab
tab
df <- as.data.frame(untab$x)
colnames(df)<- c('Uninteded')
df$Intended <- tab$x
df
? prop.test
prop.test(as.matrix(df))
df
colSums(df)
as.list(colSums(df))
prop.test(df[2,],colsums(df))
prop.test(df[2,],colSums(df))
levels(rd$Question)
unintended.bp <- droplevels(unintended[unintended$Question == 'Indicator of infant currently alive',])
levels(unintended.bp$QuestionId)
unintended.bp <- droplevels(unintended[unintended$QuestionId == 'QUO143',])
intended.bp <- droplevels(intended[intended$QuestionId == 'QUO143',])
untab <- aggregate(unintended.bp$Sample_Size, by = list(Response = unintended.bp$Response), FUN = sum)
tab <- aggregate(intended.bp$Sample_Size, by = list(Response = intended.bp$Response), FUN = sum)
x<-c(untab[2,2],tab[2,2])
dfun<-as.data.frame(untab)
as.numeric(dfun$x)
df<-as.data.frame(tab)
as.numeric(df$x)
n<-c(sum(dfun$x),sum(df$x))
prop.test(x,n)
levels(rd$Question)
untab
tab
unintended.bp <- droplevels(unintended[unintended$QuestionId == 'QUO143',])
intended.bp <- droplevels(intended[intended$QuestionId == 'QUO143',])
untab <- aggregate(unintended.bp$Sample_Size, by = list(Response = unintended.bp$Response), FUN = sum)
tab <- aggregate(intended.bp$Sample_Size, by = list(Response = intended.bp$Response), FUN = sum)
x<-c(untab[2,2],tab[2,2])
dfun<-as.data.frame(untab)
as.numeric(dfun$x)
df<-as.data.frame(tab)
as.numeric(df$x)
n<-c(sum(dfun$x),sum(df$x))
prop.test(x,n)
untab
tab
x
n
unintended.bp <- droplevels(unintended[unintended$QuestionId == 'QUO143',])
intended.bp <- droplevels(intended[intended$QuestionId == 'QUO143',])
untab <- aggregate(unintended.bp$Sample_Size, by = list(Response = unintended.bp$Response), FUN = sum)
tab <- aggregate(intended.bp$Sample_Size, by = list(Response = intended.bp$Response), FUN = sum)
x<-c(untab[2,2],tab[2,2])
dfun<-as.data.frame(untab)
as.numeric(dfun$x)
df<-as.data.frame(tab)
as.numeric(df$x)
n<-c(sum(dfun$x),sum(df$x))
prop.test(x,n)
View(rd)
View(rd)
colnames(rd)
levels(unintended.bp$Question)
tab
