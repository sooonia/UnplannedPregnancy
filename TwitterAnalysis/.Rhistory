string<-paste(string,'+',x[i])
}
#This function searches twitter for tweets containing all requested terms from the last 10 days
tw<-NULL
if(together==1){
for(i in 1:length(x)){
new.tweets<-search_tweets(x[i],n=n,full_text='extended')
if(clean>0){
for(j in 1:nrow(new.tweets)){
str<-as.character(new.tweets[j,5])
str <- gsub('[[:punct:]]','',str)
str<-replace_emoji_identifier(str,emoji_dt = lexicon::hash_emojis_identifier)
str<-replace_emoji(str,emoji_dt = lexicon::hash_emojis)
str<-replace_emoticon(str, emoticon_dt = lexicon::hash_emoticons)
str<-gsub("<...","",str)
str<-gsub(" http.*","",str)
new.tweets[j,5]<-str
}
}
tw<-rbind(tw,new.tweets)
if(nrow(new.tweets)==n){print(paste('more tweets of: ',x[i],' may be possible'))}
}}else{
tw<-search_tweets(string,n=n,full_text='extended')
if(clean>0){
for(j in 1:nrow(tw)){
str<-as.character(tw[j,5])
str <- gsub('[[:punct:]]','',str)
str<-replace_emoji_identifier(str,emoji_dt = lexicon::hash_emojis_identifier)
str<-replace_emoji(str,emoji_dt = lexicon::hash_emojis)
str<-replace_emoticon(str, emoticon_dt = lexicon::hash_emoticons)
str<-gsub("<...","",str)
str<-gsub(" http.*","",str)
tw[j,5]<-str
}
}
}
View(tw)
dim(tw)
x<-c('Planned Parenthood','#IStandWithPP')
#do you want to search for each term separately or together? 1 is yes 0 is no
together<-0
#do you want the tweets cleaned of punctuation and make everything lowercase
clean<-1
string<-x[1]
for(i in 2:length(x)){
string<-paste(string,'+',x[i])
}
#This function searches twitter for tweets containing all requested terms from the last 10 days
tw<-NULL
if(together==1){
for(i in 1:length(x)){
new.tweets<-search_tweets(x[i],n=n,full_text='extended')
if(clean>0){
for(j in 1:nrow(new.tweets)){
str<-as.character(new.tweets[j,5])
str <- gsub('[[:punct:]]','',str)
str<-replace_emoji_identifier(str,emoji_dt = lexicon::hash_emojis_identifier)
str<-replace_emoji(str,emoji_dt = lexicon::hash_emojis)
str<-replace_emoticon(str, emoticon_dt = lexicon::hash_emoticons)
str<-gsub("<...","",str)
str<-gsub(" http.*","",str)
new.tweets[j,5]<-str
}
}
tw<-rbind(tw,new.tweets)
if(nrow(new.tweets)==n){print(paste('more tweets of: ',x[i],' may be possible'))}
}}else{
tw<-search_tweets(string,n=n,full_text='extended')
if(clean>0){
for(j in 1:nrow(tw)){
str<-as.character(tw[j,5])
str <- gsub('[[:punct:]]','',str)
str<-replace_emoji_identifier(str,emoji_dt = lexicon::hash_emojis_identifier)
str<-replace_emoji(str,emoji_dt = lexicon::hash_emojis)
str<-replace_emoticon(str, emoticon_dt = lexicon::hash_emoticons)
str<-gsub("<...","",str)
str<-gsub(" http.*","",str)
tw[j,5]<-str
}
}
}
View(tw)
dim(tw)
head(tw$text)
getData <- function(n= 10000, x){
#this x is the requested terms to search for separated by a comma and in quotes
#do you want to search for each term separately or together? 1 is yes 0 is no
together<-0
#do you want the tweets cleaned of punctuation and make everything lowercase
clean<-1
string<-x[1]
for(i in 2:length(x)){
string<-paste(string,'+',x[i])
}
#This function searches twitter for tweets containing all requested terms from the last 10 days
tw<-NULL
if(together==1){
for(i in 1:length(x)){
new.tweets<-search_tweets(x[i],n=n,full_text='extended')
if(clean>0){
for(j in 1:nrow(new.tweets)){
str<-as.character(new.tweets[j,5])
str <- gsub('[[:punct:]]','',str)
str<-replace_emoji_identifier(str,emoji_dt = lexicon::hash_emojis_identifier)
str<-replace_emoji(str,emoji_dt = lexicon::hash_emojis)
str<-replace_emoticon(str, emoticon_dt = lexicon::hash_emoticons)
str<-gsub("<...","",str)
str<-gsub(" http.*","",str)
new.tweets[j,5]<-str
}
}
tw<-rbind(tw,new.tweets)
if(nrow(new.tweets)==n){print(paste('more tweets of: ',x[i],' may be possible'))}
}}else{
tw<-search_tweets(string,n=n,full_text='extended')
if(clean>0){
for(j in 1:nrow(tw)){
str<-as.character(tw[j,5])
str <- gsub('[[:punct:]]','',str)
str<-replace_emoji_identifier(str,emoji_dt = lexicon::hash_emojis_identifier)
str<-replace_emoji(str,emoji_dt = lexicon::hash_emojis)
str<-replace_emoticon(str, emoticon_dt = lexicon::hash_emoticons)
str<-gsub("<...","",str)
str<-gsub(" http.*","",str)
tw[j,5]<-str
}
}
}
return(tw)
}
tw <- getData(x)
x <- c('#IstandwithPP', 'planned parenthood')
n <- 10000
tw <- getData(n, x)
View(tw)
dim(tw)
x <- c('#IstandwithPP', 'planned parenthood')
n <- 10000
tw <- rbind(tw,getData(n, x))
dim(tw)
tw <- []
remove(tw)
x <- c('nfl')
n <- 10000
tw <- rbind(tw,getData(n, x))
dim(tw)
tw <- getData(n, x)
colnames(tw)
dim(tw)
summary(tw$lang)
tw$lang <- as.factor(tw$lang)
summary(tw$lang)
remove(tw)
x <- c('#plannedparenthood', '#pp')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('planned parenthood', '#pp')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
head(tw$text)
together<- 0
library(textclean)
library(rtweet)
library(httpuv)
library(twitteR)
#DO NOT GIVE OUT THE FOLLOWING INFORMATION TO ANYONE
consumer_key <- 	'3bxyiuG3DJzmXlUvAF6NRQpv5'
consumer_secret <- 'i3Td2j5iQToDCcSTm0kLshb6vIIETutoT2YHYY40kbPt9oqr4V'
access_token <- "953818124243087360-kEijMPCzceyjWH3D9Hf8BSImq9K3ihp"
access_secret <- "GjDCLKyT1klUeRVJoEsSH74z7tFt8fEc5cj9Ks3YYp6vs"
#This authorizes your computer to receive the information from twitter
# This should only be run once per R session
twitter_tokens <- create_token(app = "API Tester App for MAT 552",consumer_key = consumer_key, consumer_secret = consumer_secret)
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#Call to twitter for tweets containing specific hashtags or words
#n is the number of tweets requested for each term
x <- c('planned parenthood', '#pp')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
getData <- function(n= 10000, x){
#this x is the requested terms to search for separated by a comma and in quotes
#do you want to search for each term separately or together? 1 is yes 0 is no
together<- 0
#do you want the tweets cleaned of punctuation and make everything lowercase
clean<-1
string<-x[1]
for(i in 2:length(x)){
string<-paste(string,'+',x[i])
}
#This function searches twitter for tweets containing all requested terms from the last 10 days
tw<-NULL
if(together==1){
for(i in 1:length(x)){
new.tweets<-search_tweets(x[i],n=n,full_text='extended')
if(clean>0){
for(j in 1:nrow(new.tweets)){
str<-as.character(new.tweets[j,5])
str <- gsub('[[:punct:]]','',str)
str<-replace_emoji_identifier(str,emoji_dt = lexicon::hash_emojis_identifier)
str<-replace_emoji(str,emoji_dt = lexicon::hash_emojis)
str<-replace_emoticon(str, emoticon_dt = lexicon::hash_emoticons)
str<-gsub("<...","",str)
str<-gsub(" http.*","",str)
new.tweets[j,5]<-str
}
}
tw<-rbind(tw,new.tweets)
if(nrow(new.tweets)==n){print(paste('more tweets of: ',x[i],' may be possible'))}
}}else{
tw<-search_tweets(string,n=n,full_text='extended')
if(clean>0){
for(j in 1:nrow(tw)){
str<-as.character(tw[j,5])
str <- gsub('[[:punct:]]','',str)
str<-replace_emoji_identifier(str,emoji_dt = lexicon::hash_emojis_identifier)
str<-replace_emoji(str,emoji_dt = lexicon::hash_emojis)
str<-replace_emoticon(str, emoticon_dt = lexicon::hash_emoticons)
str<-gsub("<...","",str)
str<-gsub(" http.*","",str)
tw[j,5]<-str
}
}
}
return(tw)
}
consumer_key <- 	'3bxyiuG3DJzmXlUvAF6NRQpv5'
consumer_secret <- 'i3Td2j5iQToDCcSTm0kLshb6vIIETutoT2YHYY40kbPt9oqr4V'
access_token <- "953818124243087360-kEijMPCzceyjWH3D9Hf8BSImq9K3ihp"
access_secret <- "GjDCLKyT1klUeRVJoEsSH74z7tFt8fEc5cj9Ks3YYp6vs"
#This authorizes your computer to receive the information from twitter
# This should only be run once per R session
twitter_tokens <- create_token(app = "API Tester App for MAT 552",consumer_key = consumer_key, consumer_secret = consumer_secret)
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#Call to twitter for tweets containing specific hashtags or words
#n is the number of tweets requested for each term
x <- c('planned parenthood', '#pp')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('planned parenthood OR #pp OR #standwithpp')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
remove(tw)
x <- c('planned parenthood OR #pp OR #standwithpp')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
tw
clear
clear()
dim(tw)
x <- c('planned parenthood OR #pp')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('planned parenthood OR #pp OR #standwithpp')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('planned parenthood OR #pp OR #standwithpp OR pp OR pregnancy')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
pi
9/16
8/9
x <- c('planned parenthood OR #pp OR #standwithpp OR pp OR pregnancy OR #IstandwithPP')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('planned parenthood OR pp OR #standwithpp OR pp OR pregnancy OR #IstandwithPP')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('pp')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
x <- c('pp OR planned parenthood')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('pp OR planned parenthood')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('pp', 'planned parenthood')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
x <- c('#pp', 'istandwithpp', 'standwithpp', 'planned parenthood')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('#pp', 'istandwithpp')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('#pp', '#istandwithpp', '#standwithpp', 'planned parenthood')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('pregnancy')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
x <- c('pregnancy OR planned parenthood OR trump')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
getData <- function(n= 10000, x){
#this x is the requested terms to search for separated by a comma and in quotes
#do you want the tweets cleaned of punctuation and make everything lowercase
clean<-1
string<-x[1]
for(i in 2:length(x)){
string<-paste(string,'+',x[i])
}
#This function searches twitter for tweets containing all requested terms from the last 10 days
tw<-NULL
new.tweets<-search_tweets(x,n=n,full_text='extended')
if(clean>0){
for(j in 1:nrow(new.tweets)){
str<-as.character(new.tweets[j,5])
str <- gsub('[[:punct:]]','',str)
str<-replace_emoji_identifier(str,emoji_dt = lexicon::hash_emojis_identifier)
str<-replace_emoji(str,emoji_dt = lexicon::hash_emojis)
str<-replace_emoticon(str, emoticon_dt = lexicon::hash_emoticons)
str<-gsub("<...","",str)
str<-gsub(" http.*","",str)
new.tweets[j,5]<-str
}
}
tw<-rbind(tw,new.tweets)
if(nrow(new.tweets)==n){print(paste('more tweets of: ',x[i],' may be possible'))}
return(tw)
}
#DO NOT GIVE OUT THE FOLLOWING INFORMATION TO ANYONE
consumer_key <- 	'3bxyiuG3DJzmXlUvAF6NRQpv5'
consumer_secret <- 'i3Td2j5iQToDCcSTm0kLshb6vIIETutoT2YHYY40kbPt9oqr4V'
access_token <- "953818124243087360-kEijMPCzceyjWH3D9Hf8BSImq9K3ihp"
access_secret <- "GjDCLKyT1klUeRVJoEsSH74z7tFt8fEc5cj9Ks3YYp6vs"
#This authorizes your computer to receive the information from twitter
# This should only be run once per R session
twitter_tokens <- create_token(app = "API Tester App for MAT 552",consumer_key = consumer_key, consumer_secret = consumer_secret)
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#Call to twitter for tweets containing specific hashtags or words
#n is the number of tweets requested for each term
x <- c('pregnancy OR planned parenthood OR trump')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
x <- c('pregnancy OR planned parenthood OR #istandwithpp OR #standwithpp')
n <- 10000
#tw <- rbind(tw,getData(n, x))
tw <- getData(n, x)
dim(tw)
library(tm)
library(topicmodels)
library(SnowballC)
load('tweets.rda')
useful.vars <- c('text', 'source', 'is_retweet', 'favorite_count', 'retweet_count', 'hashtags', 'lang')
tweets <- tw[useful.vars]
corpus <- Corpus(VectorSource(tweets$text))
#cleaning
#remove stopwords
corpus <- tm_map(corpus, content_transformer(removeWords), stopwords("english"))
#lowercase
corpus <- tm_map(corpus, content_transformer(tolower))
#stem words --- good idea??
#corpus <- tm_map(corpus, stemDocument)
#strip whitespace
corpus <- tm_map(corpus, stripWhitespace)
#tweets$text.clean <- data.frame(text=sapply(corpus, identity), stringsAsFactors=F)$text
dtm <- DocumentTermMatrix(corpus)
rowTotals <- apply(dtm , 1, sum)
dtm   <- dtm[rowTotals> 0, ]
ld <- LDA(dtm, k = 20)
terms(ld, 5)
setwd("~/GitHub/UnplannedPregnancy/TwitterAnalysis")
load('tweets.rda')
useful.vars <- c('text', 'source', 'is_retweet', 'favorite_count', 'retweet_count', 'hashtags', 'lang')
tweets <- tw[useful.vars]
corpus <- Corpus(VectorSource(tweets$text))
#cleaning
#remove stopwords
corpus <- tm_map(corpus, content_transformer(removeWords), stopwords("english"))
#lowercase
corpus <- tm_map(corpus, content_transformer(tolower))
#stem words --- good idea??
#corpus <- tm_map(corpus, stemDocument)
#strip whitespace
corpus <- tm_map(corpus, stripWhitespace)
#tweets$text.clean <- data.frame(text=sapply(corpus, identity), stringsAsFactors=F)$text
dtm <- DocumentTermMatrix(corpus)
rowTotals <- apply(dtm , 1, sum)
dtm   <- dtm[rowTotals> 0, ]
ld <- LDA(dtm, k = 20)
terms(ld, 5)
tops <- topics(ld)
tweets[1,]
preg <- tweets[tweets$text %like% "preg" , ]
preg <- tweets[grep("preg", tweets$text , ]
preg <- tweets[grep("preg", tweets$text) , ]
View(preg)
preg[1,1]
preg[3,1]
tops <- as.data.frame(topics(ld))
tweets$topic_num <- as.data.frame(topics(ld))
tweets$text.clean <- data.frame(text=sapply(corpus, identity), stringsAsFactors=F)$text
rowTotals <- apply(tweets$text.clean , 1, sum)
tweets <- tweets[rowTotals> 0, ]
tweets$topic_num <- as.data.frame(topics(ld))
install.package('Rsentiment')
install.package('RSentiment')
install.packages('RSentiment')
library(RSentiment)
score <- calculate_score(tweets$text.clean)
dim(tweets)
dim(tweets)[1]
tweets$score <- as.data.frame(matrix(0, dim(tweets)[1], 1))
summary(tweets$score)
tweets$score <- as.array(matrix(0, dim(tweets)[1], 1))
tweets$score
head(tweets$score)
tweets <- tw[useful.vars]
tweets$topic_num <- as.data.frame(topics(ld))
tweets$text.clean <- data.frame(text=sapply(corpus, identity), stringsAsFactors=F)$text
tweets <- tweets[rowTotals> 0, ]
tweets$topic_num <- as.data.frame(topics(ld))
summary(tweets$topic_num)
temp <- as.data.frame(topics(ld))
temp <- as.data.frame(matrix(0, dim(tweets)[1], 1))
tweets$topic_num <- as.data.frame(topics(ld))$`topics(ld)`
tweets$score <- as.data.frame(matrix(0, dim(tweets)[1], 1))$V1
i=1
sub <- tweets[tweets$topic_num == i,]
score <- calculate_score(sub$text.clean)
score <- as.data.frame(score)
tweets[tweets$topic_num == 1, 'score'] <- as.data.frame(score)$score
summary(tweets$score)
for (i in 1:num_topics){
sub <- tweets[tweets$topic_num == i,]
score <- calculate_score(sub$text.clean)
tweets[tweets$topic_num == 1, 'score'] <- as.data.frame(score)$score
}
num_topics <- 20
tweets$score <- as.data.frame(matrix(0, dim(tweets)[1], 1))$V1
for (i in 1:num_topics){
sub <- tweets[tweets$topic_num == i,]
score <- calculate_score(sub$text.clean)
tweets[tweets$topic_num == 1, 'score'] <- as.data.frame(score)$score
}
for (i in 1:num_topics){
sub <- tweets[tweets$topic_num == i,]
score <- calculate_score(sub$text.clean)
tweets[tweets$topic_num == i, 'score'] <- as.data.frame(score)$score
}
dim(tweets)[1]/100
save(tweets, file = 'tweets_an.rda')
load('tweets_an.rda')
for (i in 1:num_topics){
sub <- tweets[tweets$topic_num == i,]
score <- calculate_score(sub$text.clean)
tweets[tweets$topic_num == i, 'score'] <- as.data.frame(score)$score
}
num_topics <- 20
tweets$score <- as.data.frame(matrix(0, dim(tweets)[1], 1))$V1
for (i in 1:num_topics){
sub <- tweets[tweets$topic_num == i,]
score <- calculate_score(sub$text.clean)
tweets[tweets$topic_num == i, 'score'] <- as.data.frame(score)$score
}
summary(tweets$score)
colnames(tweets)
load('tweets_an.rda')
tweets$score <- as.data.frame(matrix(0, dim(tweets)[1], 1))$V1
for (i in 1:dim(tweets)[1]) {
score <- calculate_score(tweets[i,'text.clean'])
tweets[i, 'score'] <- as.data.frame(score)$score
}
for (i in 9050:dim(tweets)[1]) {
score <- calculate_score(tweets[i,'text.clean'])
tweets[i, 'score'] <- as.data.frame(score)$score
}
save(tweets, file = 'tweets_an.rda')
load('tweets_an.rda')
for (i in 9050:dim(tweets)[1]) {
score <- calculate_score(tweets[i,'text.clean'])
tweets[i, 'score'] <- as.data.frame(score)$score
}
tweets[10781,10]
save(tweets, file = 'tweets_an.rda')
